<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content=" 最近AI大模型非常流行、中国也产生了许多属于自己的AI大模型，例如DeepSeek-V3等，作为程序员，我也打算了解一下AI大模型究竟是什么原理，居然可以通过对话的形式告诉你答案。\n源码拉取 源码地址：https://github.com/deepseek-ai/DeepSeek-V3.git\n官方论文 地址：https://arxiv.org/pdf/2412.19437\n项目布局分析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # 项目目录结构 . ├── README.md # 核心说明文档 │ ├── 模型架构说明（MLA注意力/负载均衡策略） │ ├── 多框架支持（SGLang/LMDeploy/vLLM等） │ ├── 性能基准（代码/数学/中英文任务） │ └── 商用许可（MIT License） │ └── inference/ # 推理核心模块 ├── convert.py # 权重格式转换工具 │ ├── 参数: --n-experts 256 --model-parallel 16 │ └── 关键技术：MoE分片/参数名映射 │ ├── fp8_cast_bf16.py # 精度转换工具 │ ├── 功能: FP8→BF16转换 │ └── 特性: 动态缩放因子管理/内存优化 │ ├── generate.py # 分布式推理入口 │ ├── 交互式聊天接口（>>> 提示符） │ ├── 批量处理（--input-file参数） │ └── 多节点通信（torchrun启动） │ ├── kernel.py # 高性能计算核 │ ├── FP8矩阵乘法（Triton实现） │ ├── 权重反量化核（block_size=128） │ └── 激活量化函数 │ ├── model.py # 模型架构定义 │ ├── Transformer主类（片段25） │ ├── MLA注意力（分离式位置编码） │ └── MoE门控（专家分组选择） │ └── configs/ # 模型配置目录 ├── config_16B.json # 轻量级配置（RTX 4090等） ├── config_236B.json # 中规模配置（企业级单机） └── config_671B.json # 全量配置（多节点分布式） README阅读 Introduction We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n"><title>DeepSeek-V3学习</title><link rel=canonical href=https://crownjoker07.github.io/p/deepseek-v3%E5%AD%A6%E4%B9%A0/><link rel=stylesheet href=/scss/style.min.6a692fd055deae459f2a9767f57f3855ba80cafd5041317f24f7360f6ca47cdf.css><meta property='og:title' content="DeepSeek-V3学习"><meta property='og:description' content=" 最近AI大模型非常流行、中国也产生了许多属于自己的AI大模型，例如DeepSeek-V3等，作为程序员，我也打算了解一下AI大模型究竟是什么原理，居然可以通过对话的形式告诉你答案。\n源码拉取 源码地址：https://github.com/deepseek-ai/DeepSeek-V3.git\n官方论文 地址：https://arxiv.org/pdf/2412.19437\n项目布局分析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # 项目目录结构 . ├── README.md # 核心说明文档 │ ├── 模型架构说明（MLA注意力/负载均衡策略） │ ├── 多框架支持（SGLang/LMDeploy/vLLM等） │ ├── 性能基准（代码/数学/中英文任务） │ └── 商用许可（MIT License） │ └── inference/ # 推理核心模块 ├── convert.py # 权重格式转换工具 │ ├── 参数: --n-experts 256 --model-parallel 16 │ └── 关键技术：MoE分片/参数名映射 │ ├── fp8_cast_bf16.py # 精度转换工具 │ ├── 功能: FP8→BF16转换 │ └── 特性: 动态缩放因子管理/内存优化 │ ├── generate.py # 分布式推理入口 │ ├── 交互式聊天接口（>>> 提示符） │ ├── 批量处理（--input-file参数） │ └── 多节点通信（torchrun启动） │ ├── kernel.py # 高性能计算核 │ ├── FP8矩阵乘法（Triton实现） │ ├── 权重反量化核（block_size=128） │ └── 激活量化函数 │ ├── model.py # 模型架构定义 │ ├── Transformer主类（片段25） │ ├── MLA注意力（分离式位置编码） │ └── MoE门控（专家分组选择） │ └── configs/ # 模型配置目录 ├── config_16B.json # 轻量级配置（RTX 4090等） ├── config_236B.json # 中规模配置（企业级单机） └── config_671B.json # 全量配置（多节点分布式） README阅读 Introduction We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n"><meta property='og:url' content='https://crownjoker07.github.io/p/deepseek-v3%E5%AD%A6%E4%B9%A0/'><meta property='og:site_name' content='庄泽伟的博客'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2025-04-19T14:37:00+08:00'><meta property='article:modified_time' content='2025-04-19T14:37:00+08:00'><meta name=twitter:title content="DeepSeek-V3学习"><meta name=twitter:description content=" 最近AI大模型非常流行、中国也产生了许多属于自己的AI大模型，例如DeepSeek-V3等，作为程序员，我也打算了解一下AI大模型究竟是什么原理，居然可以通过对话的形式告诉你答案。\n源码拉取 源码地址：https://github.com/deepseek-ai/DeepSeek-V3.git\n官方论文 地址：https://arxiv.org/pdf/2412.19437\n项目布局分析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # 项目目录结构 . ├── README.md # 核心说明文档 │ ├── 模型架构说明（MLA注意力/负载均衡策略） │ ├── 多框架支持（SGLang/LMDeploy/vLLM等） │ ├── 性能基准（代码/数学/中英文任务） │ └── 商用许可（MIT License） │ └── inference/ # 推理核心模块 ├── convert.py # 权重格式转换工具 │ ├── 参数: --n-experts 256 --model-parallel 16 │ └── 关键技术：MoE分片/参数名映射 │ ├── fp8_cast_bf16.py # 精度转换工具 │ ├── 功能: FP8→BF16转换 │ └── 特性: 动态缩放因子管理/内存优化 │ ├── generate.py # 分布式推理入口 │ ├── 交互式聊天接口（>>> 提示符） │ ├── 批量处理（--input-file参数） │ └── 多节点通信（torchrun启动） │ ├── kernel.py # 高性能计算核 │ ├── FP8矩阵乘法（Triton实现） │ ├── 权重反量化核（block_size=128） │ └── 激活量化函数 │ ├── model.py # 模型架构定义 │ ├── Transformer主类（片段25） │ ├── MLA注意力（分离式位置编码） │ └── MoE门控（专家分组选择） │ └── configs/ # 模型配置目录 ├── config_16B.json # 轻量级配置（RTX 4090等） ├── config_236B.json # 中规模配置（企业级单机） └── config_671B.json # 全量配置（多节点分布式） README阅读 Introduction We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n"><link rel="shortcut icon" href=/favicon/favicon.ico><script async src=/js/vendors/mermaid.min.js></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_c9cbf7fb784b760a.png width=300 height=299 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>😄</span></figure><div class=site-meta><h1 class=site-name><a href=/>庄泽伟的博客</a></h1><h2 class=site-description>一只乌鸦的自白</h2></div></header><ol class=menu-social><li><a href=https://space.bilibili.com/106612965 target=_blank title=Bilibili rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="800" height="800" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18.223 3.086a1.25 1.25.0 010 1.768L17.08 5.996h1.17A3.75 3.75.0 0122 9.747v7.5a3.75 3.75.0 01-3.75 3.75H5.75A3.75 3.75.0 012 17.247v-7.5a3.75 3.75.0 013.75-3.75h1.166L5.775 4.855a1.25 1.25.0 111.767-1.768l2.652 2.652c.079.079.145.165.198.257h3.213c.053-.092.12-.18.199-.258l2.651-2.652a1.25 1.25.0 011.768.0zm.027 5.42H5.75A1.25 1.25.0 004.503 9.663l-.003.094v7.5c0 .659.51 1.199 1.157 1.246l.093.004h12.5a1.25 1.25.0 001.247-1.157l.003-.093v-7.5c0-.69-.56-1.25-1.25-1.25zm-10 2.5c.69.0 1.25.56 1.25 1.25v1.25a1.25 1.25.0 11-2.5.0v-1.25c0-.69.56-1.25 1.25-1.25zm7.5.0c.69.0 1.25.56 1.25 1.25v1.25a1.25 1.25.0 11-2.5.0v-1.25c0-.69.56-1.25 1.25-1.25z"/></svg></a></li><li><a href=https://github.com/CrownJoker07 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>文章</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>项目</span></a></li><li><a href=/personal/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>个人</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><ol><li><a href=#源码拉取>源码拉取</a></li><li><a href=#官方论文>官方论文</a></li><li><a href=#项目布局分析>项目布局分析</a></li><li><a href=#readme阅读>README阅读</a><ol><li><a href=#introduction>Introduction</a></li><li><a href=#model-summary>Model Summary</a></li></ol></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/aillm/>AILLM</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/deepseek-v3%E5%AD%A6%E4%B9%A0/>DeepSeek-V3学习</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Apr 19, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 3 分钟</time></div></footer></div></header><section class=article-content><blockquote><p>最近AI大模型非常流行、中国也产生了许多属于自己的AI大模型，例如DeepSeek-V3等，作为程序员，我也打算了解一下AI大模型究竟是什么原理，居然可以通过对话的形式告诉你答案。</p></blockquote><h3 id=源码拉取>源码拉取</h3><p>源码地址：<a class=link href=https://github.com/deepseek-ai/DeepSeek-V3.git target=_blank rel=noopener>https://github.com/deepseek-ai/DeepSeek-V3.git</a></p><h3 id=官方论文>官方论文</h3><p>地址：<a class=link href=https://arxiv.org/pdf/2412.19437 target=_blank rel=noopener>https://arxiv.org/pdf/2412.19437</a></p><h3 id=项目布局分析>项目布局分析</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl># 项目目录结构
</span></span><span class=line><span class=cl>.
</span></span><span class=line><span class=cl>├── README.md                 # 核心说明文档
</span></span><span class=line><span class=cl>│   ├── 模型架构说明（MLA注意力/负载均衡策略）
</span></span><span class=line><span class=cl>│   ├── 多框架支持（SGLang/LMDeploy/vLLM等）
</span></span><span class=line><span class=cl>│   ├── 性能基准（代码/数学/中英文任务）
</span></span><span class=line><span class=cl>│   └── 商用许可（MIT License）
</span></span><span class=line><span class=cl>│
</span></span><span class=line><span class=cl>└── inference/                # 推理核心模块
</span></span><span class=line><span class=cl>    ├── convert.py            # 权重格式转换工具
</span></span><span class=line><span class=cl>    │   ├── 参数: --n-experts 256 --model-parallel 16
</span></span><span class=line><span class=cl>    │   └── 关键技术：MoE分片/参数名映射
</span></span><span class=line><span class=cl>    │
</span></span><span class=line><span class=cl>    ├── fp8_cast_bf16.py      # 精度转换工具
</span></span><span class=line><span class=cl>    │   ├── 功能: FP8→BF16转换
</span></span><span class=line><span class=cl>    │   └── 特性: 动态缩放因子管理/内存优化
</span></span><span class=line><span class=cl>    │
</span></span><span class=line><span class=cl>    ├── generate.py           # 分布式推理入口
</span></span><span class=line><span class=cl>    │   ├── 交互式聊天接口（&gt;&gt;&gt; 提示符）
</span></span><span class=line><span class=cl>    │   ├── 批量处理（--input-file参数）
</span></span><span class=line><span class=cl>    │   └── 多节点通信（torchrun启动）
</span></span><span class=line><span class=cl>    │
</span></span><span class=line><span class=cl>    ├── kernel.py             # 高性能计算核
</span></span><span class=line><span class=cl>    │   ├── FP8矩阵乘法（Triton实现）
</span></span><span class=line><span class=cl>    │   ├── 权重反量化核（block_size=128）
</span></span><span class=line><span class=cl>    │   └── 激活量化函数
</span></span><span class=line><span class=cl>    │
</span></span><span class=line><span class=cl>    ├── model.py              # 模型架构定义
</span></span><span class=line><span class=cl>    │   ├── Transformer主类（片段25）
</span></span><span class=line><span class=cl>    │   ├── MLA注意力（分离式位置编码）
</span></span><span class=line><span class=cl>    │   └── MoE门控（专家分组选择）
</span></span><span class=line><span class=cl>    │
</span></span><span class=line><span class=cl>    └── configs/              # 模型配置目录
</span></span><span class=line><span class=cl>        ├── config_16B.json   # 轻量级配置（RTX 4090等）
</span></span><span class=line><span class=cl>        ├── config_236B.json  # 中规模配置（企业级单机）
</span></span><span class=line><span class=cl>        └── config_671B.json  # 全量配置（多节点分布式）
</span></span></code></pre></td></tr></table></div></div><h3 id=readme阅读>README阅读</h3><h4 id=introduction>Introduction</h4><blockquote><p>We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.</p></blockquote><p><strong>关键点</strong>：</p><ul><li>Mixture-of-Experts (MoE):</li></ul><div class=mermaid align=center>flowchart TD
A[输入 Input<br>e.g. 文本Token序列] --> B[门控网络 Gating Network]
subgraph C [专家网络 Experts<br>并行计算池]
C1[Expert 1]
C2[Expert 2]
C3[Expert 3]
C4[Expert ...]
end
B -- Top-K选择<br>（e.g. K=2） --> C1
B -- Top-K选择<br>（e.g. K=2） --> C2
B --> C3
B --> C4
C1 -- 输出1 --> D{组合函数<br>Combining Function}
C2 -- 输出2 --> D
C3 --> E[丢弃]
C4 --> E
D -- 加权求和 --> F[最终输出 Output]</div><p>神经网络架构设计，旨在构建更大规模的模型而不显著增加计算成本。它的核心思想是"专家分工"。</p><ul><li><p>Multi-head Latent Attention (MLA)</p></li><li><p>auxiliary-loss-free strategy</p></li><li><p>multi-token prediction</p></li></ul><p>评测基准：</p><ul><li>MMLU（Massive Multitask Language Understanding，大规模多任务语言理解）：<br>一个涵盖 57 个主题的多项选择题基准，用于评估大规模语言模型的知识和推理能力。包括基本数学、美国历史、计算机科学、法律等多个领域。</li><li>MMLU Pro：<br>MMLU 的专业级别版本，包含更具挑战性的问题，旨在评估模型在专业领域的理解和推理能力。</li><li>GPQA-Diamond（Grade-Level Problems in Question Answering）：<br>旨在提供一个全面的框架，能够测试模型在多种推理场景下的能力，并推动大模型在更加复杂任务上的改进。</li><li>MATH-500：<br>OpenAI从MATH评测数据集中精选的500个更具代表性的数学评测基准</li><li>AIME 2024（American Invitational Mathematics Examination）：<br>美国数学邀请赛，是美国面向中学生的邀请式竞赛，3个小时完成15道题，难度很高。</li><li>SWE-bench（Software Engineering Bench）：<br>一个从GitHub上提炼的真实世界的Python代码仓的任务评测数据集</li><li>SWE-bench Verified：<br>OpenAI基于SWE-Bench提炼的更加准确和更具代表性的大模型代码工程任务解决能力评测</li></ul><h4 id=model-summary>Model Summary</h4><hr><p>Architecture: Innovative Load Balancing Strategy and Training Objective</p><p>On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.
We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. It can also be used for speculative decoding for inference acceleration.</p></section><footer class=article-footer><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><script src=https://utteranc.es/client.js repo=CrownJoker07/CrownJoker07.github.io issue-term=pathname label=Comment crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2025 ZhuangZewei</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>
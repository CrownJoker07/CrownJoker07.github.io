<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content=" æœ€è¿‘AIå¤§æ¨¡å‹éå¸¸æµè¡Œã€ä¸­å›½ä¹Ÿäº§ç”Ÿäº†è®¸å¤šå±äºè‡ªå·±çš„AIå¤§æ¨¡å‹ï¼Œä¾‹å¦‚DeepSeek-V3ç­‰ï¼Œä½œä¸ºç¨‹åºå‘˜ï¼Œæˆ‘ä¹Ÿæ‰“ç®—äº†è§£ä¸€ä¸‹AIå¤§æ¨¡å‹ç©¶ç«Ÿæ˜¯ä»€ä¹ˆåŸç†ï¼Œå±…ç„¶å¯ä»¥é€šè¿‡å¯¹è¯çš„å½¢å¼å‘Šè¯‰ä½ ç­”æ¡ˆã€‚\næºç æ‹‰å– æºç åœ°å€ï¼šhttps://github.com/deepseek-ai/DeepSeek-V3.git\nå®˜æ–¹è®ºæ–‡ åœ°å€ï¼šhttps://arxiv.org/pdf/2412.19437\né¡¹ç›®å¸ƒå±€åˆ†æ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # é¡¹ç›®ç›®å½•ç»“æ„ . â”œâ”€â”€ README.md # æ ¸å¿ƒè¯´æ˜æ–‡æ¡£ â”‚ â”œâ”€â”€ æ¨¡å‹æ¶æ„è¯´æ˜ï¼ˆMLAæ³¨æ„åŠ›/è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼‰ â”‚ â”œâ”€â”€ å¤šæ¡†æ¶æ”¯æŒï¼ˆSGLang/LMDeploy/vLLMç­‰ï¼‰ â”‚ â”œâ”€â”€ æ€§èƒ½åŸºå‡†ï¼ˆä»£ç /æ•°å­¦/ä¸­è‹±æ–‡ä»»åŠ¡ï¼‰ â”‚ â””â”€â”€ å•†ç”¨è®¸å¯ï¼ˆMIT Licenseï¼‰ â”‚ â””â”€â”€ inference/ # æ¨ç†æ ¸å¿ƒæ¨¡å— â”œâ”€â”€ convert.py # æƒé‡æ ¼å¼è½¬æ¢å·¥å…· â”‚ â”œâ”€â”€ å‚æ•°: --n-experts 256 --model-parallel 16 â”‚ â””â”€â”€ å…³é”®æŠ€æœ¯ï¼šMoEåˆ†ç‰‡/å‚æ•°åæ˜ å°„ â”‚ â”œâ”€â”€ fp8_cast_bf16.py # ç²¾åº¦è½¬æ¢å·¥å…· â”‚ â”œâ”€â”€ åŠŸèƒ½: FP8â†’BF16è½¬æ¢ â”‚ â””â”€â”€ ç‰¹æ€§: åŠ¨æ€ç¼©æ”¾å› å­ç®¡ç†/å†…å­˜ä¼˜åŒ– â”‚ â”œâ”€â”€ generate.py # åˆ†å¸ƒå¼æ¨ç†å…¥å£ â”‚ â”œâ”€â”€ äº¤äº’å¼èŠå¤©æ¥å£ï¼ˆ>>> æç¤ºç¬¦ï¼‰ â”‚ â”œâ”€â”€ æ‰¹é‡å¤„ç†ï¼ˆ--input-fileå‚æ•°ï¼‰ â”‚ â””â”€â”€ å¤šèŠ‚ç‚¹é€šä¿¡ï¼ˆtorchrunå¯åŠ¨ï¼‰ â”‚ â”œâ”€â”€ kernel.py # é«˜æ€§èƒ½è®¡ç®—æ ¸ â”‚ â”œâ”€â”€ FP8çŸ©é˜µä¹˜æ³•ï¼ˆTritonå®ç°ï¼‰ â”‚ â”œâ”€â”€ æƒé‡åé‡åŒ–æ ¸ï¼ˆblock_size=128ï¼‰ â”‚ â””â”€â”€ æ¿€æ´»é‡åŒ–å‡½æ•° â”‚ â”œâ”€â”€ model.py # æ¨¡å‹æ¶æ„å®šä¹‰ â”‚ â”œâ”€â”€ Transformerä¸»ç±»ï¼ˆç‰‡æ®µ25ï¼‰ â”‚ â”œâ”€â”€ MLAæ³¨æ„åŠ›ï¼ˆåˆ†ç¦»å¼ä½ç½®ç¼–ç ï¼‰ â”‚ â””â”€â”€ MoEé—¨æ§ï¼ˆä¸“å®¶åˆ†ç»„é€‰æ‹©ï¼‰ â”‚ â””â”€â”€ configs/ # æ¨¡å‹é…ç½®ç›®å½• â”œâ”€â”€ config_16B.json # è½»é‡çº§é…ç½®ï¼ˆRTX 4090ç­‰ï¼‰ â”œâ”€â”€ config_236B.json # ä¸­è§„æ¨¡é…ç½®ï¼ˆä¼ä¸šçº§å•æœºï¼‰ â””â”€â”€ config_671B.json # å…¨é‡é…ç½®ï¼ˆå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼ï¼‰ READMEé˜…è¯» Introduction We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n"><title>DeepSeek-V3å­¦ä¹ </title><link rel=canonical href=https://crownjoker07.github.io/p/deepseek-v3%E5%AD%A6%E4%B9%A0/><link rel=stylesheet href=/scss/style.min.6a692fd055deae459f2a9767f57f3855ba80cafd5041317f24f7360f6ca47cdf.css><meta property='og:title' content="DeepSeek-V3å­¦ä¹ "><meta property='og:description' content=" æœ€è¿‘AIå¤§æ¨¡å‹éå¸¸æµè¡Œã€ä¸­å›½ä¹Ÿäº§ç”Ÿäº†è®¸å¤šå±äºè‡ªå·±çš„AIå¤§æ¨¡å‹ï¼Œä¾‹å¦‚DeepSeek-V3ç­‰ï¼Œä½œä¸ºç¨‹åºå‘˜ï¼Œæˆ‘ä¹Ÿæ‰“ç®—äº†è§£ä¸€ä¸‹AIå¤§æ¨¡å‹ç©¶ç«Ÿæ˜¯ä»€ä¹ˆåŸç†ï¼Œå±…ç„¶å¯ä»¥é€šè¿‡å¯¹è¯çš„å½¢å¼å‘Šè¯‰ä½ ç­”æ¡ˆã€‚\næºç æ‹‰å– æºç åœ°å€ï¼šhttps://github.com/deepseek-ai/DeepSeek-V3.git\nå®˜æ–¹è®ºæ–‡ åœ°å€ï¼šhttps://arxiv.org/pdf/2412.19437\né¡¹ç›®å¸ƒå±€åˆ†æ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # é¡¹ç›®ç›®å½•ç»“æ„ . â”œâ”€â”€ README.md # æ ¸å¿ƒè¯´æ˜æ–‡æ¡£ â”‚ â”œâ”€â”€ æ¨¡å‹æ¶æ„è¯´æ˜ï¼ˆMLAæ³¨æ„åŠ›/è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼‰ â”‚ â”œâ”€â”€ å¤šæ¡†æ¶æ”¯æŒï¼ˆSGLang/LMDeploy/vLLMç­‰ï¼‰ â”‚ â”œâ”€â”€ æ€§èƒ½åŸºå‡†ï¼ˆä»£ç /æ•°å­¦/ä¸­è‹±æ–‡ä»»åŠ¡ï¼‰ â”‚ â””â”€â”€ å•†ç”¨è®¸å¯ï¼ˆMIT Licenseï¼‰ â”‚ â””â”€â”€ inference/ # æ¨ç†æ ¸å¿ƒæ¨¡å— â”œâ”€â”€ convert.py # æƒé‡æ ¼å¼è½¬æ¢å·¥å…· â”‚ â”œâ”€â”€ å‚æ•°: --n-experts 256 --model-parallel 16 â”‚ â””â”€â”€ å…³é”®æŠ€æœ¯ï¼šMoEåˆ†ç‰‡/å‚æ•°åæ˜ å°„ â”‚ â”œâ”€â”€ fp8_cast_bf16.py # ç²¾åº¦è½¬æ¢å·¥å…· â”‚ â”œâ”€â”€ åŠŸèƒ½: FP8â†’BF16è½¬æ¢ â”‚ â””â”€â”€ ç‰¹æ€§: åŠ¨æ€ç¼©æ”¾å› å­ç®¡ç†/å†…å­˜ä¼˜åŒ– â”‚ â”œâ”€â”€ generate.py # åˆ†å¸ƒå¼æ¨ç†å…¥å£ â”‚ â”œâ”€â”€ äº¤äº’å¼èŠå¤©æ¥å£ï¼ˆ>>> æç¤ºç¬¦ï¼‰ â”‚ â”œâ”€â”€ æ‰¹é‡å¤„ç†ï¼ˆ--input-fileå‚æ•°ï¼‰ â”‚ â””â”€â”€ å¤šèŠ‚ç‚¹é€šä¿¡ï¼ˆtorchrunå¯åŠ¨ï¼‰ â”‚ â”œâ”€â”€ kernel.py # é«˜æ€§èƒ½è®¡ç®—æ ¸ â”‚ â”œâ”€â”€ FP8çŸ©é˜µä¹˜æ³•ï¼ˆTritonå®ç°ï¼‰ â”‚ â”œâ”€â”€ æƒé‡åé‡åŒ–æ ¸ï¼ˆblock_size=128ï¼‰ â”‚ â””â”€â”€ æ¿€æ´»é‡åŒ–å‡½æ•° â”‚ â”œâ”€â”€ model.py # æ¨¡å‹æ¶æ„å®šä¹‰ â”‚ â”œâ”€â”€ Transformerä¸»ç±»ï¼ˆç‰‡æ®µ25ï¼‰ â”‚ â”œâ”€â”€ MLAæ³¨æ„åŠ›ï¼ˆåˆ†ç¦»å¼ä½ç½®ç¼–ç ï¼‰ â”‚ â””â”€â”€ MoEé—¨æ§ï¼ˆä¸“å®¶åˆ†ç»„é€‰æ‹©ï¼‰ â”‚ â””â”€â”€ configs/ # æ¨¡å‹é…ç½®ç›®å½• â”œâ”€â”€ config_16B.json # è½»é‡çº§é…ç½®ï¼ˆRTX 4090ç­‰ï¼‰ â”œâ”€â”€ config_236B.json # ä¸­è§„æ¨¡é…ç½®ï¼ˆä¼ä¸šçº§å•æœºï¼‰ â””â”€â”€ config_671B.json # å…¨é‡é…ç½®ï¼ˆå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼ï¼‰ READMEé˜…è¯» Introduction We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n"><meta property='og:url' content='https://crownjoker07.github.io/p/deepseek-v3%E5%AD%A6%E4%B9%A0/'><meta property='og:site_name' content='åº„æ³½ä¼Ÿçš„åšå®¢'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2025-04-19T14:37:00+08:00'><meta property='article:modified_time' content='2025-04-19T14:37:00+08:00'><meta name=twitter:title content="DeepSeek-V3å­¦ä¹ "><meta name=twitter:description content=" æœ€è¿‘AIå¤§æ¨¡å‹éå¸¸æµè¡Œã€ä¸­å›½ä¹Ÿäº§ç”Ÿäº†è®¸å¤šå±äºè‡ªå·±çš„AIå¤§æ¨¡å‹ï¼Œä¾‹å¦‚DeepSeek-V3ç­‰ï¼Œä½œä¸ºç¨‹åºå‘˜ï¼Œæˆ‘ä¹Ÿæ‰“ç®—äº†è§£ä¸€ä¸‹AIå¤§æ¨¡å‹ç©¶ç«Ÿæ˜¯ä»€ä¹ˆåŸç†ï¼Œå±…ç„¶å¯ä»¥é€šè¿‡å¯¹è¯çš„å½¢å¼å‘Šè¯‰ä½ ç­”æ¡ˆã€‚\næºç æ‹‰å– æºç åœ°å€ï¼šhttps://github.com/deepseek-ai/DeepSeek-V3.git\nå®˜æ–¹è®ºæ–‡ åœ°å€ï¼šhttps://arxiv.org/pdf/2412.19437\né¡¹ç›®å¸ƒå±€åˆ†æ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # é¡¹ç›®ç›®å½•ç»“æ„ . â”œâ”€â”€ README.md # æ ¸å¿ƒè¯´æ˜æ–‡æ¡£ â”‚ â”œâ”€â”€ æ¨¡å‹æ¶æ„è¯´æ˜ï¼ˆMLAæ³¨æ„åŠ›/è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼‰ â”‚ â”œâ”€â”€ å¤šæ¡†æ¶æ”¯æŒï¼ˆSGLang/LMDeploy/vLLMç­‰ï¼‰ â”‚ â”œâ”€â”€ æ€§èƒ½åŸºå‡†ï¼ˆä»£ç /æ•°å­¦/ä¸­è‹±æ–‡ä»»åŠ¡ï¼‰ â”‚ â””â”€â”€ å•†ç”¨è®¸å¯ï¼ˆMIT Licenseï¼‰ â”‚ â””â”€â”€ inference/ # æ¨ç†æ ¸å¿ƒæ¨¡å— â”œâ”€â”€ convert.py # æƒé‡æ ¼å¼è½¬æ¢å·¥å…· â”‚ â”œâ”€â”€ å‚æ•°: --n-experts 256 --model-parallel 16 â”‚ â””â”€â”€ å…³é”®æŠ€æœ¯ï¼šMoEåˆ†ç‰‡/å‚æ•°åæ˜ å°„ â”‚ â”œâ”€â”€ fp8_cast_bf16.py # ç²¾åº¦è½¬æ¢å·¥å…· â”‚ â”œâ”€â”€ åŠŸèƒ½: FP8â†’BF16è½¬æ¢ â”‚ â””â”€â”€ ç‰¹æ€§: åŠ¨æ€ç¼©æ”¾å› å­ç®¡ç†/å†…å­˜ä¼˜åŒ– â”‚ â”œâ”€â”€ generate.py # åˆ†å¸ƒå¼æ¨ç†å…¥å£ â”‚ â”œâ”€â”€ äº¤äº’å¼èŠå¤©æ¥å£ï¼ˆ>>> æç¤ºç¬¦ï¼‰ â”‚ â”œâ”€â”€ æ‰¹é‡å¤„ç†ï¼ˆ--input-fileå‚æ•°ï¼‰ â”‚ â””â”€â”€ å¤šèŠ‚ç‚¹é€šä¿¡ï¼ˆtorchrunå¯åŠ¨ï¼‰ â”‚ â”œâ”€â”€ kernel.py # é«˜æ€§èƒ½è®¡ç®—æ ¸ â”‚ â”œâ”€â”€ FP8çŸ©é˜µä¹˜æ³•ï¼ˆTritonå®ç°ï¼‰ â”‚ â”œâ”€â”€ æƒé‡åé‡åŒ–æ ¸ï¼ˆblock_size=128ï¼‰ â”‚ â””â”€â”€ æ¿€æ´»é‡åŒ–å‡½æ•° â”‚ â”œâ”€â”€ model.py # æ¨¡å‹æ¶æ„å®šä¹‰ â”‚ â”œâ”€â”€ Transformerä¸»ç±»ï¼ˆç‰‡æ®µ25ï¼‰ â”‚ â”œâ”€â”€ MLAæ³¨æ„åŠ›ï¼ˆåˆ†ç¦»å¼ä½ç½®ç¼–ç ï¼‰ â”‚ â””â”€â”€ MoEé—¨æ§ï¼ˆä¸“å®¶åˆ†ç»„é€‰æ‹©ï¼‰ â”‚ â””â”€â”€ configs/ # æ¨¡å‹é…ç½®ç›®å½• â”œâ”€â”€ config_16B.json # è½»é‡çº§é…ç½®ï¼ˆRTX 4090ç­‰ï¼‰ â”œâ”€â”€ config_236B.json # ä¸­è§„æ¨¡é…ç½®ï¼ˆä¼ä¸šçº§å•æœºï¼‰ â””â”€â”€ config_671B.json # å…¨é‡é…ç½®ï¼ˆå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼ï¼‰ READMEé˜…è¯» Introduction We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n"><link rel="shortcut icon" href=/favicon/favicon.ico><script async src=/js/vendors/mermaid.min.js></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=åˆ‡æ¢èœå•>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_c9cbf7fb784b760a.png width=300 height=299 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>ğŸ˜„</span></figure><div class=site-meta><h1 class=site-name><a href=/>åº„æ³½ä¼Ÿçš„åšå®¢</a></h1><h2 class=site-description>ä¸€åªä¹Œé¸¦çš„è‡ªç™½</h2></div></header><ol class=menu-social><li><a href=https://space.bilibili.com/106612965 target=_blank title=Bilibili rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="800" height="800" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18.223 3.086a1.25 1.25.0 010 1.768L17.08 5.996h1.17A3.75 3.75.0 0122 9.747v7.5a3.75 3.75.0 01-3.75 3.75H5.75A3.75 3.75.0 012 17.247v-7.5a3.75 3.75.0 013.75-3.75h1.166L5.775 4.855a1.25 1.25.0 111.767-1.768l2.652 2.652c.079.079.145.165.198.257h3.213c.053-.092.12-.18.199-.258l2.651-2.652a1.25 1.25.0 011.768.0zm.027 5.42H5.75A1.25 1.25.0 004.503 9.663l-.003.094v7.5c0 .659.51 1.199 1.157 1.246l.093.004h12.5a1.25 1.25.0 001.247-1.157l.003-.093v-7.5c0-.69-.56-1.25-1.25-1.25zm-10 2.5c.69.0 1.25.56 1.25 1.25v1.25a1.25 1.25.0 11-2.5.0v-1.25c0-.69.56-1.25 1.25-1.25zm7.5.0c.69.0 1.25.56 1.25 1.25v1.25a1.25 1.25.0 11-2.5.0v-1.25c0-.69.56-1.25 1.25-1.25z"/></svg></a></li><li><a href=https://github.com/CrownJoker07 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>ä¸»é¡µ</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>æ–‡ç« </span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>æœç´¢</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>é¡¹ç›®</span></a></li><li><a href=/personal/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>ä¸ªäºº</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>æš—è‰²æ¨¡å¼</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">ç›®å½•</h2><div class=widget--toc><nav id=TableOfContents><ol><li><ol><li><a href=#æºç æ‹‰å–>æºç æ‹‰å–</a></li><li><a href=#å®˜æ–¹è®ºæ–‡>å®˜æ–¹è®ºæ–‡</a></li><li><a href=#é¡¹ç›®å¸ƒå±€åˆ†æ>é¡¹ç›®å¸ƒå±€åˆ†æ</a></li><li><a href=#readmeé˜…è¯»>READMEé˜…è¯»</a><ol><li><a href=#introduction>Introduction</a></li><li><a href=#model-summary>Model Summary</a></li></ol></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/aillm/>AILLM</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/deepseek-v3%E5%AD%A6%E4%B9%A0/>DeepSeek-V3å­¦ä¹ </a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Apr 19, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>é˜…è¯»æ—¶é•¿: 3 åˆ†é’Ÿ</time></div></footer></div></header><section class=article-content><blockquote><p>æœ€è¿‘AIå¤§æ¨¡å‹éå¸¸æµè¡Œã€ä¸­å›½ä¹Ÿäº§ç”Ÿäº†è®¸å¤šå±äºè‡ªå·±çš„AIå¤§æ¨¡å‹ï¼Œä¾‹å¦‚DeepSeek-V3ç­‰ï¼Œä½œä¸ºç¨‹åºå‘˜ï¼Œæˆ‘ä¹Ÿæ‰“ç®—äº†è§£ä¸€ä¸‹AIå¤§æ¨¡å‹ç©¶ç«Ÿæ˜¯ä»€ä¹ˆåŸç†ï¼Œå±…ç„¶å¯ä»¥é€šè¿‡å¯¹è¯çš„å½¢å¼å‘Šè¯‰ä½ ç­”æ¡ˆã€‚</p></blockquote><h3 id=æºç æ‹‰å–>æºç æ‹‰å–</h3><p>æºç åœ°å€ï¼š<a class=link href=https://github.com/deepseek-ai/DeepSeek-V3.git target=_blank rel=noopener>https://github.com/deepseek-ai/DeepSeek-V3.git</a></p><h3 id=å®˜æ–¹è®ºæ–‡>å®˜æ–¹è®ºæ–‡</h3><p>åœ°å€ï¼š<a class=link href=https://arxiv.org/pdf/2412.19437 target=_blank rel=noopener>https://arxiv.org/pdf/2412.19437</a></p><h3 id=é¡¹ç›®å¸ƒå±€åˆ†æ>é¡¹ç›®å¸ƒå±€åˆ†æ</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl># é¡¹ç›®ç›®å½•ç»“æ„
</span></span><span class=line><span class=cl>.
</span></span><span class=line><span class=cl>â”œâ”€â”€ README.md                 # æ ¸å¿ƒè¯´æ˜æ–‡æ¡£
</span></span><span class=line><span class=cl>â”‚   â”œâ”€â”€ æ¨¡å‹æ¶æ„è¯´æ˜ï¼ˆMLAæ³¨æ„åŠ›/è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼‰
</span></span><span class=line><span class=cl>â”‚   â”œâ”€â”€ å¤šæ¡†æ¶æ”¯æŒï¼ˆSGLang/LMDeploy/vLLMç­‰ï¼‰
</span></span><span class=line><span class=cl>â”‚   â”œâ”€â”€ æ€§èƒ½åŸºå‡†ï¼ˆä»£ç /æ•°å­¦/ä¸­è‹±æ–‡ä»»åŠ¡ï¼‰
</span></span><span class=line><span class=cl>â”‚   â””â”€â”€ å•†ç”¨è®¸å¯ï¼ˆMIT Licenseï¼‰
</span></span><span class=line><span class=cl>â”‚
</span></span><span class=line><span class=cl>â””â”€â”€ inference/                # æ¨ç†æ ¸å¿ƒæ¨¡å—
</span></span><span class=line><span class=cl>    â”œâ”€â”€ convert.py            # æƒé‡æ ¼å¼è½¬æ¢å·¥å…·
</span></span><span class=line><span class=cl>    â”‚   â”œâ”€â”€ å‚æ•°: --n-experts 256 --model-parallel 16
</span></span><span class=line><span class=cl>    â”‚   â””â”€â”€ å…³é”®æŠ€æœ¯ï¼šMoEåˆ†ç‰‡/å‚æ•°åæ˜ å°„
</span></span><span class=line><span class=cl>    â”‚
</span></span><span class=line><span class=cl>    â”œâ”€â”€ fp8_cast_bf16.py      # ç²¾åº¦è½¬æ¢å·¥å…·
</span></span><span class=line><span class=cl>    â”‚   â”œâ”€â”€ åŠŸèƒ½: FP8â†’BF16è½¬æ¢
</span></span><span class=line><span class=cl>    â”‚   â””â”€â”€ ç‰¹æ€§: åŠ¨æ€ç¼©æ”¾å› å­ç®¡ç†/å†…å­˜ä¼˜åŒ–
</span></span><span class=line><span class=cl>    â”‚
</span></span><span class=line><span class=cl>    â”œâ”€â”€ generate.py           # åˆ†å¸ƒå¼æ¨ç†å…¥å£
</span></span><span class=line><span class=cl>    â”‚   â”œâ”€â”€ äº¤äº’å¼èŠå¤©æ¥å£ï¼ˆ&gt;&gt;&gt; æç¤ºç¬¦ï¼‰
</span></span><span class=line><span class=cl>    â”‚   â”œâ”€â”€ æ‰¹é‡å¤„ç†ï¼ˆ--input-fileå‚æ•°ï¼‰
</span></span><span class=line><span class=cl>    â”‚   â””â”€â”€ å¤šèŠ‚ç‚¹é€šä¿¡ï¼ˆtorchrunå¯åŠ¨ï¼‰
</span></span><span class=line><span class=cl>    â”‚
</span></span><span class=line><span class=cl>    â”œâ”€â”€ kernel.py             # é«˜æ€§èƒ½è®¡ç®—æ ¸
</span></span><span class=line><span class=cl>    â”‚   â”œâ”€â”€ FP8çŸ©é˜µä¹˜æ³•ï¼ˆTritonå®ç°ï¼‰
</span></span><span class=line><span class=cl>    â”‚   â”œâ”€â”€ æƒé‡åé‡åŒ–æ ¸ï¼ˆblock_size=128ï¼‰
</span></span><span class=line><span class=cl>    â”‚   â””â”€â”€ æ¿€æ´»é‡åŒ–å‡½æ•°
</span></span><span class=line><span class=cl>    â”‚
</span></span><span class=line><span class=cl>    â”œâ”€â”€ model.py              # æ¨¡å‹æ¶æ„å®šä¹‰
</span></span><span class=line><span class=cl>    â”‚   â”œâ”€â”€ Transformerä¸»ç±»ï¼ˆç‰‡æ®µ25ï¼‰
</span></span><span class=line><span class=cl>    â”‚   â”œâ”€â”€ MLAæ³¨æ„åŠ›ï¼ˆåˆ†ç¦»å¼ä½ç½®ç¼–ç ï¼‰
</span></span><span class=line><span class=cl>    â”‚   â””â”€â”€ MoEé—¨æ§ï¼ˆä¸“å®¶åˆ†ç»„é€‰æ‹©ï¼‰
</span></span><span class=line><span class=cl>    â”‚
</span></span><span class=line><span class=cl>    â””â”€â”€ configs/              # æ¨¡å‹é…ç½®ç›®å½•
</span></span><span class=line><span class=cl>        â”œâ”€â”€ config_16B.json   # è½»é‡çº§é…ç½®ï¼ˆRTX 4090ç­‰ï¼‰
</span></span><span class=line><span class=cl>        â”œâ”€â”€ config_236B.json  # ä¸­è§„æ¨¡é…ç½®ï¼ˆä¼ä¸šçº§å•æœºï¼‰
</span></span><span class=line><span class=cl>        â””â”€â”€ config_671B.json  # å…¨é‡é…ç½®ï¼ˆå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼ï¼‰
</span></span></code></pre></td></tr></table></div></div><h3 id=readmeé˜…è¯»>READMEé˜…è¯»</h3><h4 id=introduction>Introduction</h4><blockquote><p>We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.</p></blockquote><p><strong>å…³é”®ç‚¹</strong>ï¼š</p><ul><li>Mixture-of-Experts (MoE):</li></ul><div class=mermaid align=center>flowchart TD
A[è¾“å…¥ Input<br>e.g. æ–‡æœ¬Tokenåºåˆ—] --> B[é—¨æ§ç½‘ç»œ Gating Network]
subgraph C [ä¸“å®¶ç½‘ç»œ Experts<br>å¹¶è¡Œè®¡ç®—æ± ]
C1[Expert 1]
C2[Expert 2]
C3[Expert 3]
C4[Expert ...]
end
B -- Top-Ké€‰æ‹©<br>ï¼ˆe.g. K=2ï¼‰ --> C1
B -- Top-Ké€‰æ‹©<br>ï¼ˆe.g. K=2ï¼‰ --> C2
B --> C3
B --> C4
C1 -- è¾“å‡º1 --> D{ç»„åˆå‡½æ•°<br>Combining Function}
C2 -- è¾“å‡º2 --> D
C3 --> E[ä¸¢å¼ƒ]
C4 --> E
D -- åŠ æƒæ±‚å’Œ --> F[æœ€ç»ˆè¾“å‡º Output]</div><p>ç¥ç»ç½‘ç»œæ¶æ„è®¾è®¡ï¼Œæ—¨åœ¨æ„å»ºæ›´å¤§è§„æ¨¡çš„æ¨¡å‹è€Œä¸æ˜¾è‘—å¢åŠ è®¡ç®—æˆæœ¬ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯"ä¸“å®¶åˆ†å·¥"ã€‚</p><ul><li><p>Multi-head Latent Attention (MLA)</p></li><li><p>auxiliary-loss-free strategy</p></li><li><p>multi-token prediction</p></li></ul><p>è¯„æµ‹åŸºå‡†ï¼š</p><ul><li>MMLUï¼ˆMassive Multitask Language Understandingï¼Œå¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼‰ï¼š<br>ä¸€ä¸ªæ¶µç›– 57 ä¸ªä¸»é¢˜çš„å¤šé¡¹é€‰æ‹©é¢˜åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚åŒ…æ‹¬åŸºæœ¬æ•°å­¦ã€ç¾å›½å†å²ã€è®¡ç®—æœºç§‘å­¦ã€æ³•å¾‹ç­‰å¤šä¸ªé¢†åŸŸã€‚</li><li>MMLU Proï¼š<br>MMLU çš„ä¸“ä¸šçº§åˆ«ç‰ˆæœ¬ï¼ŒåŒ…å«æ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸçš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚</li><li>GPQA-Diamondï¼ˆGrade-Level Problems in Question Answeringï¼‰ï¼š<br>æ—¨åœ¨æä¾›ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿæµ‹è¯•æ¨¡å‹åœ¨å¤šç§æ¨ç†åœºæ™¯ä¸‹çš„èƒ½åŠ›ï¼Œå¹¶æ¨åŠ¨å¤§æ¨¡å‹åœ¨æ›´åŠ å¤æ‚ä»»åŠ¡ä¸Šçš„æ”¹è¿›ã€‚</li><li>MATH-500ï¼š<br>OpenAIä»MATHè¯„æµ‹æ•°æ®é›†ä¸­ç²¾é€‰çš„500ä¸ªæ›´å…·ä»£è¡¨æ€§çš„æ•°å­¦è¯„æµ‹åŸºå‡†</li><li>AIME 2024ï¼ˆAmerican Invitational Mathematics Examinationï¼‰ï¼š<br>ç¾å›½æ•°å­¦é‚€è¯·èµ›ï¼Œæ˜¯ç¾å›½é¢å‘ä¸­å­¦ç”Ÿçš„é‚€è¯·å¼ç«èµ›ï¼Œ3ä¸ªå°æ—¶å®Œæˆ15é“é¢˜ï¼Œéš¾åº¦å¾ˆé«˜ã€‚</li><li>SWE-benchï¼ˆSoftware Engineering Benchï¼‰ï¼š<br>ä¸€ä¸ªä»GitHubä¸Šæç‚¼çš„çœŸå®ä¸–ç•Œçš„Pythonä»£ç ä»“çš„ä»»åŠ¡è¯„æµ‹æ•°æ®é›†</li><li>SWE-bench Verifiedï¼š<br>OpenAIåŸºäºSWE-Benchæç‚¼çš„æ›´åŠ å‡†ç¡®å’Œæ›´å…·ä»£è¡¨æ€§çš„å¤§æ¨¡å‹ä»£ç å·¥ç¨‹ä»»åŠ¡è§£å†³èƒ½åŠ›è¯„æµ‹</li></ul><h4 id=model-summary>Model Summary</h4><hr><p>Architecture: Innovative Load Balancing Strategy and Training Objective</p><p>On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.
We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. It can also be used for speculative decoding for inference acceleration.</p></section><footer class=article-footer><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><script src=https://utteranc.es/client.js repo=CrownJoker07/CrownJoker07.github.io issue-term=pathname label=Comment crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2025 ZhuangZewei</section><section class=powerby>ä½¿ç”¨ <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> æ„å»º<br>ä¸»é¢˜ <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> ç”± <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> è®¾è®¡</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>